# -*- coding: utf-8 -*-
"""mek_cnn_odev.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hw1DX751Lo7Ga7UPtUIRX-FCGKkdaPa7
"""

import keras
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D

import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import random
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, classification_report
import itertools

(X_train_org, y_train_org), (X_test_org, y_test_org) = cifar10.load_data()
X_train_org, X_val, y_train_org, y_val = train_test_split( X_train_org, y_train_org, test_size=0.2, random_state=42,stratify=y_train_org)
print('X_train shape:', X_train_org.shape)
print('y_train shape:', y_train_org.shape)
print('X_val shape:', X_val.shape)
print('y_val shape:', y_val.shape)

print('Train Data Size:',X_train_org.shape[0])
print('Validation Data Size',X_val.shape[0])
print('Test Data Size',X_test_org.shape[0])

X_train = X_train_org
X_test = X_test_org
y_train = y_train_org
y_test = y_test_org

"""# Random Image"""

random_no = random.randint(0,9)

plt.figure()
plt.imshow(X_train[random_no])
plt.colorbar()

unique, counts = np.unique(y_train, return_counts=True)
dict(zip(unique, counts))

unique, counts = np.unique(y_test, return_counts=True)
dict(zip(unique, counts))

unique, counts = np.unique(y_val, return_counts=True)
dict(zip(unique, counts))

"""Normalize Data"""

# 0-255 arası piksel değerlerinin normalizasyonu
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_val = X_val.astype('float32')

X_train /= 255
X_test /= 255
X_val /= 255

"""Label Encoding"""

num_of_classes = 10
y_train_encoded = to_categorical(y_train,num_of_classes)
y_test_encoded = to_categorical(y_test,num_of_classes)
y_val_encoded = to_categorical(y_val,num_of_classes)

y_train=y_train_encoded
y_test=y_test_encoded
y_val=y_val_encoded

X_train.shape[1:]

"""Create A Model"""

def plotmodelhistory(history): 
    fig, axs = plt.subplots(1,2,figsize=(15,5)) 
    # summarize history for accuracy
    axs[0].plot(history.history['accuracy']) 
    axs[0].plot(history.history['val_accuracy']) 
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy') 
    axs[0].set_xlabel('Epoch')
    axs[0].legend(['train', 'validate'], loc='upper left')
    # summarize history for loss
    axs[1].plot(history.history['loss']) 
    axs[1].plot(history.history['val_loss']) 
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss') 
    axs[1].set_xlabel('Epoch')
    axs[1].legend(['train', 'validate'], loc='upper left')
    plt.show()

"""#Model_1: 3 Conv. Layers, 32 filters, (3,3)"""

model_1 = Sequential()
model_1.add(Conv2D(32,(3,3),input_shape=X_train.shape[1:],activation='relu'))
model_1.add(MaxPooling2D(pool_size=(2,2)))
model_1.add(Dropout(0.25)) 
model_1.add(Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))
model_1.add(MaxPooling2D(pool_size=(2,2)))
model_1.add(Dropout(0.25))
model_1.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu'))
model_1.add(MaxPooling2D(pool_size=(2,2)))
model_1.add(Dropout(0.25))
model_1.add(Flatten())
model_1.add(Dense(256,activation='relu'))
model_1.add(Dense(num_of_classes,activation='softmax'))
model_1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# model_1 = Sequential()
# model_1.add(Conv2D(32,(3,3),input_shape=X_train.shape[1:],activation='relu'))
# model_1.add(MaxPooling2D(pool_size=(2,2)))
# model_1.add(Dropout(0.5)) 
# model_1.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu'))
# model_1.add(MaxPooling2D(pool_size=(2,2)))
# model_1.add(Dropout(0.25))
# model_1.add(Conv2D(128,(3,3),input_shape=(32,32,3),activation='relu'))
# model_1.add(MaxPooling2D(pool_size=(2,2)))
# model_1.add(Dropout(0.25))
# model_1.add(Flatten())
# model_1.add(Dense(256,activation='relu'))
# model_1.add(Dense(num_of_classes,activation='softmax'))
# model_1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model_1.summary()

model_1.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

batch_size = 32

history = model_1.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=30,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_1.evaluate(X_val, y_val)

def plotmodelhistory(history): 
    fig, axs = plt.subplots(1,2,figsize=(15,5)) 
    # summarize history for accuracy
    axs[0].plot(history.history['accuracy']) 
    axs[0].plot(history.history['val_accuracy']) 
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy') 
    axs[0].set_xlabel('Epoch')
    axs[0].legend(['train', 'validate'], loc='upper left')
    # summarize history for loss
    axs[1].plot(history.history['loss']) 
    axs[1].plot(history.history['val_loss']) 
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss') 
    axs[1].set_xlabel('Epoch')
    axs[1].legend(['train', 'validate'], loc='upper left')
    plt.show()

# list all data in history
print(history.history.keys())

plotmodelhistory(history)

"""#Model_2: 3 Conv. Layers,32 Filters, (5,5)"""

model_2 = Sequential()
model_2.add(Conv2D(32,(5,5),input_shape=X_train.shape[1:],activation='relu'))
model_2.add(Conv2D(32,(5,5),input_shape=(32,32,3),activation='relu'))
model_2.add(MaxPooling2D(pool_size=(2,2)))
model_2.add(Dropout(0.25))
model_2.add(Conv2D(32,(5,5),input_shape=(32,32,3),activation='relu'))
model_2.add(MaxPooling2D(pool_size=(2,2)))
model_2.add(Dropout(0.25))

model_2.add(Flatten())
model_2.add(Dense(256,activation='relu'))
model_2.add(Dense(num_of_classes,activation='softmax'))
model_2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])


model_2.summary()

batch_size = 32

history = model_2.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=20,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_2.evaluate(X_val, y_val)

print(history.history.keys())

plotmodelhistory(history)

"""#Model_3: 3 Conv. Layers, 64 filters, (3,3)"""

model_3 = Sequential()
model_3.add(Conv2D(64,(3,3),input_shape=X_train.shape[1:],activation='relu',padding='same'))
model_3.add(MaxPooling2D(pool_size=(2,2)))
model_3.add(Dropout(0.25))
model_3.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu',padding='same'))
model_3.add(MaxPooling2D(pool_size=(2,2)))
model_3.add(Dropout(0.25))
model_3.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu', padding='same'))
model_3.add(MaxPooling2D(pool_size=(2,2)))
model_3.add(Dropout(0.25))


model_3.add(Flatten())
model_3.add(Dense(256,activation='relu'))
model_3.add(Dense(num_of_classes,activation='softmax'))
model_3.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])


model_3.summary()

batch_size = 32

history = model_3.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=30,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_3.evaluate(X_val, y_val)

print(history.history.keys())

plotmodelhistory(history)

"""#Model_4: 3 Conv. Layers, 64 filters, (5,5)"""

model_4 = Sequential()
model_4.add(Conv2D(64,(5,5),input_shape=X_train.shape[1:],activation='relu'))
model_4.add(Conv2D(64,(5,5),input_shape=(32,32,3),activation='relu'))
model_4.add(MaxPooling2D(pool_size=(2,2)))
model_4.add(Dropout(0.5))
model_4.add(Conv2D(64,(5,5),input_shape=(32,32,3),activation='relu'))
model_4.add(MaxPooling2D(pool_size=(2,2)))
model_4.add(Dropout(0.5))

model_4.add(Flatten())
model_4.add(Dense(256,activation='relu'))
model_4.add(Dense(num_of_classes,activation='softmax'))
model_4.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])


model_4.summary()

batch_size = 32

history = model_4.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=35,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_4.evaluate(X_val, y_val)

print(history.history.keys())

plotmodelhistory(history)

from keras.utils import plot_model
plot_model(model_4, to_file='cnn_plot.png', show_shapes=True, show_layer_names=True)

"""##5LAYERS

#Model_5: 5 Conv. Layers, 32 filters, (3,3)
"""

model_5 = Sequential()
model_5.add(Conv2D(32,(3,3),input_shape=X_train.shape[1:],activation='relu'))
model_5.add(Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))
model_5.add(MaxPooling2D(pool_size=(2,2)))
model_5.add(Dropout(0.25))
model_5.add(Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))
model_5.add(Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))
model_5.add(MaxPooling2D(pool_size=(2,2)))
model_5.add(Dropout(0.25))
model_5.add(Conv2D(32,(3,3),input_shape=(32,32,3),activation='relu'))
model_5.add(MaxPooling2D(pool_size=(2,2)))
model_5.add(Dropout(0.25))
model_5.add(Flatten())
model_5.add(Dense(256,activation='relu'))
model_5.add(Dense(num_of_classes,activation='softmax'))
model_5.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model_5.summary()

model_5.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

batch_size = 32

history = model_5.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=30,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_5.evaluate(X_val, y_val)

def plotmodelhistory(history): 
    fig, axs = plt.subplots(1,2,figsize=(15,5)) 
    # summarize history for accuracy
    axs[0].plot(history.history['accuracy']) 
    axs[0].plot(history.history['val_accuracy']) 
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy') 
    axs[0].set_xlabel('Epoch')
    axs[0].legend(['train', 'validate'], loc='upper left')
    # summarize history for loss
    axs[1].plot(history.history['loss']) 
    axs[1].plot(history.history['val_loss']) 
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss') 
    axs[1].set_xlabel('Epoch')
    axs[1].legend(['train', 'validate'], loc='upper left')
    plt.show()

# list all data in history

plotmodelhistory(history)

"""#Model_6: 5 Conv. Layers, 32 filters, (5,5)"""

model_6 = Sequential()
model_6.add(Conv2D(32,(5,5),input_shape=X_train.shape[1:],activation='relu', padding='same'))
model_6.add(MaxPooling2D(pool_size=(2,2)))
model_6.add(Dropout(0.25))
model_6.add(Conv2D(32,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_6.add(MaxPooling2D(pool_size=(2,2)))
model_6.add(Dropout(0.25))
model_6.add(Conv2D(32,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_6.add(Conv2D(32,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_6.add(MaxPooling2D(pool_size=(2,2)))
model_6.add(Dropout(0.25))
model_6.add(Conv2D(32,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_6.add(MaxPooling2D(pool_size=(2,2)))
model_6.add(Dropout(0.25))
model_6.add(Flatten())
model_6.add(Dense(256,activation='relu'))
model_6.add(Dense(num_of_classes,activation='softmax'))
model_6.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model_6.summary()

model_6.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

batch_size = 32

history = model_6.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=30,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_6.evaluate(X_val, y_val)

print(history.history.keys())

plotmodelhistory(history)

"""#Model_7: 5 Conv. Layers, 64 filters, (3,3)"""

model_7 = Sequential()
model_7.add(Conv2D(64,(3,3),input_shape=X_train.shape[1:],activation='relu', padding='same'))
model_7.add(MaxPooling2D(pool_size=(2,2)))
model_7.add(Dropout(0.5))
model_7.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu', padding='same'))
model_7.add(MaxPooling2D(pool_size=(2,2)))
model_7.add(Dropout(0.25))
model_7.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu', padding='same'))
model_7.add(MaxPooling2D(pool_size=(2,2)))
model_7.add(Dropout(0.25))
model_7.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu', padding='same'))
model_7.add(MaxPooling2D(pool_size=(2,2)))
model_7.add(Dropout(0.25))
model_7.add(Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu', padding='same'))
model_7.add(MaxPooling2D(pool_size=(2,2)))
model_7.add(Dropout(0.25))
model_7.add(Flatten())
model_7.add(Dense(256,activation='relu'))
model_7.add(Dense(num_of_classes,activation='softmax'))
model_7.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model_7.summary()

model_7.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

batch_size = 32

history = model_7.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=30,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_7.evaluate(X_val, y_val)

print(history.history.keys())

plotmodelhistory(history)

"""#Model_8: 5 Conv. Layers, 64 filters, (5,5)"""

model_8 = Sequential()
model_8.add(Conv2D(64,(5,5),input_shape=X_train.shape[1:],activation='relu', padding='same'))
model_8.add(MaxPooling2D(pool_size=(2,2)))
model_8.add(Dropout(0.5))
model_8.add(Conv2D(64,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_8.add(MaxPooling2D(pool_size=(2,2)))
model_8.add(Dropout(0.25))
model_8.add(Conv2D(64,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_8.add(Conv2D(64,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_8.add(MaxPooling2D(pool_size=(2,2)))
model_8.add(Dropout(0.25))
model_8.add(Conv2D(64,(5,5),input_shape=(32,32,3),activation='relu', padding='same'))
model_8.add(MaxPooling2D(pool_size=(2,2)))
model_8.add(Dropout(0.25))
model_8.add(Flatten())
model_8.add(Dense(256,activation='relu'))
model_8.add(Dense(num_of_classes,activation='softmax'))
model_8.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

model_8.summary()

model_8.compile(loss='categorical_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

batch_size = 32

history = model_8.fit(X_train, y_train,
              batch_size=batch_size,
              epochs=50,
              validation_data=(X_val, y_val),
              shuffle=True)

evaluation = model_8.evaluate(X_val, y_val)

print(history.history.keys())

plotmodelhistory(history)

"""# Testing For Model_4"""

evaluation = model_4.evaluate(X_test, y_test)

pred = model_4.predict(X_test)

def heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel="", **kwargs):
    """
    Create a heatmap from a numpy array and two lists of labels.
    """
    if not ax:
        ax = plt.gca()

    # Plot the heatmap
    im = ax.imshow(data, **kwargs)

    # Create colorbar
    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va="bottom")

    # Let the horizontal axes labeling appear on top.
    ax.tick_params(top=True, bottom=False,
                   labeltop=True, labelbottom=False)
    ax.set_xticks(np.arange(data.shape[1]))
    ax.set_yticks(np.arange(data.shape[0]))
    # ... and label them with the respective list entries.
    ax.set_xticklabels(col_labels)
    ax.set_yticklabels(row_labels)
    
    ax.set_xlabel('Predicted Label') 
    ax.set_ylabel('True Label')
    
    return im, cbar

def annotate_heatmap(im, data=None, fmt="d", threshold=None):
    """
    A function to annotate a heatmap.
    """
    # Change the text's color depending on the data.
    texts = []
    for i in range(data.shape[0]):
        for j in range(data.shape[1]):
            text = im.axes.text(j, i, format(data[i, j], fmt), horizontalalignment="center",
                                 color="white" if data[i, j] > thresh else "black")
            texts.append(text)

    return texts



labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']

# Convert predictions classes to one hot vectors 
Y_pred_classes = np.argmax(pred, axis=1) 
# Convert validation observations to one hot vectors
Y_true = np.argmax(y_test, axis=1)
# Errors are difference between predicted labels and true labels
errors = (Y_pred_classes - Y_true != 0)

Y_pred_classes_errors = Y_pred_classes[errors]
Y_pred_errors = pred[errors]
Y_true_errors = Y_true[errors]
X_test_errors = X_test[errors]

cm = confusion_matrix(Y_true, Y_pred_classes) 
thresh = cm.max() / 2.

fig, ax = plt.subplots(figsize=(12,12))
im, cbar = heatmap(cm, labels, labels, ax=ax,
                   cmap=plt.cm.Blues, cbarlabel="count of predictions")
lr_report = classification_report(Y_true, Y_pred_classes, target_names=labels)
print(lr_report)

texts = annotate_heatmap(im, data=cm, threshold=thresh)

fig.tight_layout()
plt.show()

"""# Test On Random Images"""

class_0=[]
class_1=[]
class_2=[]
class_3=[]
class_4=[]
class_5=[]
class_6=[]
class_7=[]
class_8=[]
class_9=[]

for index,label in enumerate(y_test_org):
   if(label==0):
      class_0.append(index)
   if(label==1):
      class_1.append(index) 
   if(label==2):
      class_2.append(index)
   if(label==3):
      class_3.append(index)    
   if(label==4):
      class_4.append(index)    
   if(label==5):
      class_5.append(index)    
   if(label==6):
      class_6.append(index)    
   if(label==7):
      class_7.append(index)
   if(label==8):
      class_8.append(index)    
   if(label==9):
      class_9.append(index)

class_0_random_img=random.choices(class_0, k=3)
class_1_random_img=random.choices(class_1, k=3)
class_2_random_img=random.choices(class_2, k=3)
class_3_random_img=random.choices(class_3, k=3)
class_4_random_img=random.choices(class_4, k=3)
class_5_random_img=random.choices(class_5, k=3)
class_6_random_img=random.choices(class_6, k=3)
class_7_random_img=random.choices(class_7, k=3)
class_8_random_img=random.choices(class_8, k=3)
class_9_random_img=random.choices(class_9, k=3)

class_0_random_img

plt.figure()
plt.imshow(X_test_org[6363])
plt.colorbar()

class_1_random_img

plt.figure()
plt.imshow(X_test_org[2694])
plt.colorbar()

class_2_random_img

plt.figure()
plt.imshow(X_test_org[9625])
plt.colorbar()

class_3_random_img

plt.figure()
plt.imshow(X_test_org[8382])
plt.colorbar()

class_4_random_img

plt.figure()
plt.imshow(X_test_org[4111])
plt.colorbar()

class_5_random_img

plt.figure()
plt.imshow(X_test_org[4172])
plt.colorbar()

class_6_random_img

plt.figure()
plt.imshow(X_test_org[7988])
plt.colorbar()

class_7_random_img

plt.figure()
plt.imshow(X_test_org[6269])
plt.colorbar()

class_8_random_img

plt.figure()
plt.imshow(X_test_org[7688])
plt.colorbar()

class_9_random_img

plt.figure()
plt.imshow(X_test_org[9845])
plt.colorbar()

all_30_img = class_0_random_img + class_1_random_img + class_2_random_img + class_3_random_img + class_4_random_img + class_5_random_img + class_6_random_img + class_7_random_img + class_8_random_img + class_9_random_img

x = np.expand_dims(X_test[class_3_random_img[1]], axis=0)

prediction =model_4.predict(x)

prediction

"""# Test Resimleri Confusion Matrix"""

true_labels = []

for i in range(10):
  for j in range(3):
    true_labels.append(i)

model_pred=model_4.predict(X_test[all_30_img])
model_pred_last = np.argmax(model_pred, axis=1)

model_pred=model_4.predict(X_test[all_30_img])
# sorting the predictions in descending order
sorting = (-model_pred).argsort()

# getting the top 2 predictions
sorted_ = sorting[29][:5]

for value in sorted_:
    # you can get your classes from the encoder(your_classes = encoder.classes_) 
    # or from a dictionary that you created before.
    # And then we access them with the predicted index.
    predicted_label = labels[value]

    # just some rounding steps
    prob = (model_pred[29][value]) * 100
    prob = "%.2f" % round(prob,2)
    print("I have %s%% sure that it belongs to %s." % (prob, predicted_label))

model_pred[0]

sorting

lr_report = classification_report(true_labels, model_pred_last, target_names=labels)
print(lr_report)

cm = confusion_matrix(true_labels, model_pred_last) 
thresh = cm.max() / 2.

fig, ax = plt.subplots(figsize=(12,12))
im, cbar = heatmap(cm, labels, labels, ax=ax,
                   cmap=plt.cm.Blues, cbarlabel="count of predictions")
texts = annotate_heatmap(im, data=cm, threshold=thresh)

fig.tight_layout()
plt.show()

